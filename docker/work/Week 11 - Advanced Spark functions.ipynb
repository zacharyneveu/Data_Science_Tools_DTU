{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkIntro\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window functions\n",
    "Last week we saw how to do group aggregations like for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+------------------+---------+\n",
      "|Pclass|         sum(Fare)|         avg(Fare)|max(Fare)|\n",
      "+------+------------------+------------------+---------+\n",
      "|     1|18177.412499999984| 84.15468749999992| 512.3292|\n",
      "|     3| 6714.695100000002|13.675550101832997|    69.55|\n",
      "|     2|3801.8416999999995| 20.66218315217391|     73.5|\n",
      "+------+------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('header', True).option('inferSchema', True).csv('data/titanic.csv')\n",
    "df.groupBy('Pclass').agg(f.sum('Fare'), f.avg('Fare'), f.max('Fare')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to compute something on a group without reducing it. For example, let's say we want to find the oldest person in each passenger class.\n",
    "\n",
    "We could first compute the max age per class and join the resulting dataframe with the original as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+----+----+-----+-----+----------+-----+-----+--------+------+\n",
      "|PassengerId|Survived|Pclass|                Name| Sex| Age|SibSp|Parch|    Ticket| Fare|Cabin|Embarked|MaxAge|\n",
      "+-----------+--------+------+--------------------+----+----+-----+-----+----------+-----+-----+--------+------+\n",
      "|        631|    true|     1|Barkworth, Mr. Al...|male|80.0|    0|    0|     27042| 30.0|  A23|       S|  80.0|\n",
      "|        673|   false|     2|Mitchell, Mr. Hen...|male|70.0|    0|    0|C.A. 24580| 10.5| null|       S|  70.0|\n",
      "|        852|   false|     3| Svensson, Mr. Johan|male|74.0|    0|    0|    347060|7.775| null|       S|  74.0|\n",
      "+-----------+--------+------+--------------------+----+----+-----+-----+----------+-----+-----+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_fare_per_class = df.groupBy('Pclass')\\\n",
    "    .agg(f.max('Age').alias('MaxAge'))\\\n",
    "    .select('MaxAge', f.col('Pclass').alias('Pclass_'))\n",
    "\n",
    "df.join(max_fare_per_class, (f.col('Age') == f.col('MaxAge')) & (f.col('Pclass') == f.col('Pclass_')), 'inner')\\\n",
    "    .drop('Pclass_')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want the three oldest passengers in each class? The method above wont work, because we are using `max` in the first dataframe.\n",
    "\n",
    "The solution is to use a Window function. With a window function you basically treat each group as a separate dataframe.\n",
    "\n",
    "In the following example we sort each group by age and add a new column with the \"rank\" of each passenger within the group. Here, the \"rank\" becomes the sort order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+----+----+-----+-----+-----------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name| Sex| Age|SibSp|Parch|     Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+----+----+-----+-----+-----------+-------+-----+--------+\n",
      "|        631|    true|     1|Barkworth, Mr. Al...|male|80.0|    0|    0|      27042|   30.0|  A23|       S|\n",
      "|         97|   false|     1|Goldschmidt, Mr. ...|male|71.0|    0|    0|   PC 17754|34.6542|   A5|       C|\n",
      "|        494|   false|     1|Artagaveytia, Mr....|male|71.0|    0|    0|   PC 17609|49.5042| null|       C|\n",
      "|        673|   false|     2|Mitchell, Mr. Hen...|male|70.0|    0|    0| C.A. 24580|   10.5| null|       S|\n",
      "|         34|   false|     2|Wheadon, Mr. Edwa...|male|66.0|    0|    0| C.A. 24579|   10.5| null|       S|\n",
      "|        571|    true|     2|  Harris, Mr. George|male|62.0|    0|    0|S.W./PP 752|   10.5| null|       S|\n",
      "|        852|   false|     3| Svensson, Mr. Johan|male|74.0|    0|    0|     347060|  7.775| null|       S|\n",
      "|        117|   false|     3|Connors, Mr. Patrick|male|70.5|    0|    0|     370369|   7.75| null|       Q|\n",
      "|        281|   false|     3|    Duane, Mr. Frank|male|65.0|    0|    0|     336439|   7.75| null|       Q|\n",
      "+-----------+--------+------+--------------------+----+----+-----+-----+-----------+-------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_by_age_window = Window.partitionBy('Pclass').orderBy(f.desc('Age'))\n",
    "\n",
    "ranked_df = df.withColumn('AgeClassRank', f.rank().over(sorted_by_age_window))\n",
    "ranked_df.filter(f.col('AgeClassRank') <= 3).drop('AgeClassRank').orderBy('Pclass', f.desc('Age')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physical Plans\n",
    "When using the DataFrame API, Spark will optimize your program by reordering the operations that you wish to perform.\n",
    "\n",
    "Lets take a look at an example from last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_traffic_df = spark.read.option('delimiter', '\\t').csv('data/traffic_2_sample')\\\n",
    "    .select(f.col('_c0').alias('ip'), f.col('_c1').alias('domain'))\n",
    "\n",
    "count_df = web_traffic_df.filter(f.col('domain') == 'google.com').groupBy('domain').agg(f.countDistinct('ip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the physical plan using `.explain()`. The physical plan is the actual plan for the work that Spark is going to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[domain#2922], functions=[count(distinct ip#2921)])\n",
      "+- Exchange hashpartitioning(domain#2922, 200)\n",
      "   +- *(2) HashAggregate(keys=[domain#2922], functions=[partial_count(distinct ip#2921)])\n",
      "      +- *(2) HashAggregate(keys=[domain#2922, ip#2921], functions=[])\n",
      "         +- Exchange hashpartitioning(domain#2922, ip#2921, 200)\n",
      "            +- *(1) HashAggregate(keys=[domain#2922, ip#2921], functions=[])\n",
      "               +- *(1) Project [_c0#2917 AS ip#2921, _c1#2918 AS domain#2922]\n",
      "                  +- *(1) Filter (isnotnull(_c1#2918) && (_c1#2918 = google.com))\n",
      "                     +- *(1) FileScan csv [_c0#2917,_c1#2918] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/data/traffic_2_sample], PartitionFilters: [], PushedFilters: [IsNotNull(_c1), EqualTo(_c1,google.com)], ReadSchema: struct<_c0:string,_c1:string>\n"
     ]
    }
   ],
   "source": [
    "count_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Spark UI ([http://127.0.0.1:4040/](http://127.0.0.1:)), you can find a graph representation of the plan under the SQL tab.\n",
    "\n",
    "The above plan has three stages (indicated by the numbers preceeding the operation name). The plan tells you that Spark will:\n",
    "\n",
    "- Read the CSV (`(1) FileScan csv`)\n",
    "- Filter rows where domain is google.com (`(1) Filter (isnotnull(_c1#2918) && (_c1#2918 = google.com)`)\n",
    "- Rename columns (`(1) Project`)\n",
    "- Shuffle the data (`Exchange hashpartitioning`)\n",
    "- Count distinct IPs on partitions (`(2) HashAggregate(keys=[domain#2922], functions=[partial_count(distinct ip#2921)])`) \n",
    "- Shuffle the aggregated data (`Exchange hashpartitioning`)\n",
    "- Sum the distinct counts (`HashAggregate(keys=[domain#2922], functions=[count(distinct ip#2921)])`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sketches in Spark\n",
    "Sketches are powerful data structures for computing estimates on count statistics in a stream. A property common to sketches is that they are composeable, i.e., two sketches obtained from two different streams can be composed to one sketch that has the same properties (error rate) as if all data was added to the same sketch.\n",
    "\n",
    "If you only need an approximate results, sketches are also useful for distributed computing. Each split of the data is considered a stream, and at the end the sketches are composed. This avoids having to shuffle the data.\n",
    "\n",
    "Consider the following example of counting unique IPs in the web traffic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|count(DISTINCT ip)|\n",
      "+------------------+\n",
      "|           9576468|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option('delimiter', '\\t').csv('data/traffic_2_sample')\\\n",
    "    .select(f.col('_c0').alias('ip'), f.col('_c1').alias('domain'))\n",
    "\n",
    "distinct_ips = df.select(f.countDistinct('ip'))\n",
    "distinct_ips.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above program triggers a shuffle `Exchange hashpartitioning`, as can be seen from the physical plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[], functions=[count(distinct ip#14)])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(2) HashAggregate(keys=[], functions=[partial_count(distinct ip#14)])\n",
      "      +- *(2) HashAggregate(keys=[ip#14], functions=[])\n",
      "         +- Exchange hashpartitioning(ip#14, 200)\n",
      "            +- *(1) HashAggregate(keys=[ip#14], functions=[])\n",
      "               +- *(1) Project [_c0#10 AS ip#14]\n",
      "                  +- *(1) FileScan csv [_c0#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/data/traffic_2_sample], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string>\n"
     ]
    }
   ],
   "source": [
    "distinct_ips.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we use `approxCountDistinct()` to approximately count instead. The physical plan does not have a shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|approx_count_distinct(ip)|\n",
      "+-------------------------+\n",
      "|                  9743877|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "approx_distinct_ips = df.select(f.approxCountDistinct('ip'))\n",
    "approx_distinct_ips.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "HashAggregate(keys=[], functions=[approx_count_distinct(ip#14, 0.05, 0, 0)])\n",
      "+- Exchange SinglePartition\n",
      "   +- HashAggregate(keys=[], functions=[partial_approx_count_distinct(ip#14, 0.05, 0, 0)])\n",
      "      +- *(1) Project [_c0#10 AS ip#14]\n",
      "         +- *(1) FileScan csv [_c0#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/data/traffic_2_sample], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string>\n"
     ]
    }
   ],
   "source": [
    "approx_distinct_ips.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: which sketch do you think Spark is using for counting distinct elements?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting and writing\n",
    "So far we have seen how to compute dataframes and print the output in the notebook. But what if we want to use the results in non-Spark programs or services?\n",
    "\n",
    "We will use the Web Traffic data to demonstrate \"collecting\" and writing to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('delimiter', '\\t').csv('data/traffic_2_sample')\\\n",
    "    .select(f.col('_c0').alias('ip'), f.col('_c1').alias('domain'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have compute a dataframe of aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_ips = df.groupBy('domain').agg(f.countDistinct('ip').alias('count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect will convert the dataframe into a list in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(domain='pandas.pydata.org', count=1266737),\n",
       " Row(domain='databricks.com', count=129125),\n",
       " Row(domain='datarobot.com', count=26387),\n",
       " Row(domain='github.com', count=129262),\n",
       " Row(domain='google.com', count=255543),\n",
       " Row(domain='scala-lang.org', count=1),\n",
       " Row(domain='wikipedia.org', count=5019691),\n",
       " Row(domain='python.org', count=2521367),\n",
       " Row(domain='dtu.dk', count=255711),\n",
       " Row(domain='spark.apache.org', count=51722)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = distinct_ips.collect()\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a list of Row objects to a list of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pandas.pydata.org', 1266737],\n",
       " ['databricks.com', 129125],\n",
       " ['datarobot.com', 26387],\n",
       " ['github.com', 129262],\n",
       " ['google.com', 255543],\n",
       " ['scala-lang.org', 1],\n",
       " ['wikipedia.org', 5019691],\n",
       " ['python.org', 2521367],\n",
       " ['dtu.dk', 255711],\n",
       " ['spark.apache.org', 51722]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda r: [r['domain'], r['count']], rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful when collecting. All data is moved to one worker, so for large dataframes it will be very slow or crash Spark. Collect is intended for being used AFTER we've used Spark to compute an aggregate dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing is as easy as reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv('data/demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark writes as many files as there are partitions. Use coalesce to write one file. But be careful, all data is sent to one worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write.csv('data/demo2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcast joins\n",
    "When you want to join two dataframes, Spark will do the following (as of Spark 2.2):\n",
    "\n",
    "- Sort the two dataframes according to the columns in the join condition\n",
    "- Locally merge join data on partitions\n",
    "\n",
    "Sorting is a very costly operation in distributed computing. It requires multiple passes over the data and a shuffle.\n",
    "\n",
    "If the dataframe on the right hand side of the join is very small, you can tell Spark to do a broadcast join, in which it will do the following:\n",
    "\n",
    "- Create a hash table mapping from the values in the columns of the join condition and send it to each worker\n",
    "- Locally hash join data on partitions\n",
    "\n",
    "The broadcast avoids the shuffle entirely.\n",
    "\n",
    "Consider the plans created by the following two Spark programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(6) Project [PassengerId#2412, Survived#2413, Pclass#2414, Name#2415, Sex#2416, Age#2417, SibSp#2418, Parch#2419, Ticket#2420, Fare#2421, Cabin#2422, Embarked#2423, MaxAge#2449]\n",
      "+- *(6) SortMergeJoin [Age#2417, Pclass#2414], [MaxAge#2449, Pclass_#2452], Inner\n",
      "   :- *(2) Sort [Age#2417 ASC NULLS FIRST, Pclass#2414 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(Age#2417, Pclass#2414, 200)\n",
      "   :     +- *(1) Project [PassengerId#2412, Survived#2413, Pclass#2414, Name#2415, Sex#2416, Age#2417, SibSp#2418, Parch#2419, Ticket#2420, Fare#2421, Cabin#2422, Embarked#2423]\n",
      "   :        +- *(1) Filter (isnotnull(Age#2417) && isnotnull(Pclass#2414))\n",
      "   :           +- *(1) FileScan csv [PassengerId#2412,Survived#2413,Pclass#2414,Name#2415,Sex#2416,Age#2417,SibSp#2418,Parch#2419,Ticket#2420,Fare#2421,Cabin#2422,Embarked#2423] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/data/titanic.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Age), IsNotNull(Pclass)], ReadSchema: struct<PassengerId:int,Survived:boolean,Pclass:int,Name:string,Sex:string,Age:double,SibSp:int,Pa...\n",
      "   +- *(5) Sort [MaxAge#2449 ASC NULLS FIRST, Pclass_#2452 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(MaxAge#2449, Pclass_#2452, 200)\n",
      "         +- *(4) Filter isnotnull(MaxAge#2449)\n",
      "            +- *(4) HashAggregate(keys=[Pclass#2414], functions=[max(Age#2417)])\n",
      "               +- Exchange hashpartitioning(Pclass#2414, 200)\n",
      "                  +- *(3) HashAggregate(keys=[Pclass#2414], functions=[partial_max(Age#2417)])\n",
      "                     +- *(3) Project [Pclass#2414, Age#2417]\n",
      "                        +- *(3) Filter isnotnull(Pclass#2414)\n",
      "                           +- *(3) FileScan csv [Pclass#2414,Age#2417] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/data/titanic.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Pclass)], ReadSchema: struct<Pclass:int,Age:double>\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "df = spark.read.option('header', True).option('inferSchema', True).csv('data/titanic.csv')\n",
    "\n",
    "max_fare_per_class = df.groupBy('Pclass')\\\n",
    "    .agg(f.max('Age').alias('MaxAge'))\\\n",
    "    .select('MaxAge', f.col('Pclass').alias('Pclass_'))\n",
    "\n",
    "df.join(max_fare_per_class, (f.col('Age') == f.col('MaxAge')) & (f.col('Pclass') == f.col('Pclass_')), 'inner')\\\n",
    "    .drop('Pclass_')\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [PassengerId#2412, Survived#2413, Pclass#2414, Name#2415, Sex#2416, Age#2417, SibSp#2418, Parch#2419, Ticket#2420, Fare#2421, Cabin#2422, Embarked#2423, MaxAge#2449]\n",
      "+- *(3) BroadcastHashJoin [Age#2417, Pclass#2414], [MaxAge#2449, Pclass_#2452], Inner, BuildRight\n",
      "   :- *(3) Project [PassengerId#2412, Survived#2413, Pclass#2414, Name#2415, Sex#2416, Age#2417, SibSp#2418, Parch#2419, Ticket#2420, Fare#2421, Cabin#2422, Embarked#2423]\n",
      "   :  +- *(3) Filter (isnotnull(Age#2417) && isnotnull(Pclass#2414))\n",
      "   :     +- *(3) FileScan csv [PassengerId#2412,Survived#2413,Pclass#2414,Name#2415,Sex#2416,Age#2417,SibSp#2418,Parch#2419,Ticket#2420,Fare#2421,Cabin#2422,Embarked#2423] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/data/titanic.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Age), IsNotNull(Pclass)], ReadSchema: struct<PassengerId:int,Survived:boolean,Pclass:int,Name:string,Sex:string,Age:double,SibSp:int,Pa...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, double, false], input[1, int, true]))\n",
      "      +- *(2) Filter isnotnull(MaxAge#2449)\n",
      "         +- *(2) HashAggregate(keys=[Pclass#2414], functions=[max(Age#2417)])\n",
      "            +- Exchange hashpartitioning(Pclass#2414, 200)\n",
      "               +- *(1) HashAggregate(keys=[Pclass#2414], functions=[partial_max(Age#2417)])\n",
      "                  +- *(1) Project [Pclass#2414, Age#2417]\n",
      "                     +- *(1) Filter isnotnull(Pclass#2414)\n",
      "                        +- *(1) FileScan csv [Pclass#2414,Age#2417] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/data/titanic.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Pclass)], ReadSchema: struct<Pclass:int,Age:double>\n"
     ]
    }
   ],
   "source": [
    "df.join(f.broadcast(max_fare_per_class), (f.col('Age') == f.col('MaxAge')) & (f.col('Pclass') == f.col('Pclass_')), 'inner')\\\n",
    "    .drop('Pclass_')\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark will automatically use the broadcast join if one of the tables is sufficiently small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
